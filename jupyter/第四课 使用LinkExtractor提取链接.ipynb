{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原版本的获取连接\n",
    "\n",
    "start_urls = ['http://books.toscrape.com/',]\n",
    "\n",
    "def parse(self, response):\n",
    "    for book_url in response.css(\"article.product_pod > h3 > a ::attr(href)\").extract():\n",
    "        yield scrapy.Request(response.urljoin(book_url), callback=self.parse_book_page)\n",
    "    # 提取下一页的链接了\n",
    "    next_page = response.css(\"li.next > a ::attr(href)\").extract_first()\n",
    "    if next_page:\n",
    "        yield scrapy.Request(response.urljoin(next_page), callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于 scrapy.linkextractors 的获取连接\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "start_urls = ['http://books.toscrape.com/',]\n",
    "\n",
    "def parse(self, response):\n",
    "    for book_url in response.css(\"article.product_pod > h3 > a ::attr(href)\").extract():\n",
    "        yield scrapy.Request(response.urljoin(book_url), callback=self.parse_book_page)\n",
    "    # 提取下一页的链接\n",
    "    le = LinkExtractor(restrict_css='ul.pager li.next')\n",
    "    links = le.extract_links(response)\n",
    "    if links:\n",
    "        next_url = links[0].url\n",
    "        yield scrapy.Request(response.urljoin(next_url), callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "htmlpath4_1 = '/home/font/Data/Scrapy/example/example4_1.html'\n",
    "htmlpath4_2 = '/home/font/Data/Scrapy/example/example4_2.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "\t<title>example4_1</title>\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "<div id=\"top\" class=\"div1\" style=\"width: 1230px\">\n",
      "\t<p>下面是是一些站内链接</p>\n",
      "\t<a class=\"internal\" href=\"/image1.html\">Name:Image 1<br/><img src=\"image1.jpg\">div_1 a_1</a>\n",
      "\t<a class=\"internal\" href=\"/image2.html\">Name:Image 2<br/><img src=\"image2.jpg\">div_1 a_2</a>\n",
      "\t<a class=\"internal\" href=\"/image3.html\">Name:Image 3<br/><img src=\"image3.jpg\">div_1 a_3</a>\n",
      "\t<a class=\"/img-red\" href=\"/image4.html\">Name:Image 4<br/><img src=\"image4.jpg\">div_1 a_4</a>\n",
      "</div>\n",
      "\n",
      "<div id=\"bottom\" style=\"width: 1230px\">\n",
      "\t<a class=\"internal\" href=\"/image5.html\">Name:Image 5<br/><img src=\"image4.jpg\"></a>\n",
      "\t<a class=\"internal\" href=\"/image6.html\">Name:Image 6<br/><img src=\"image6.jpg\"></a>\n",
      "</div>\n",
      "\n",
      "<div id=\"images-3\" style=\"width: 1230px\">\n",
      "\t<p>下面是是一些站外链接</p>\n",
      "\t<a href=\"http://blog.csdn.net/FontThrone\">Name:Image 7<br/><img src=\"image7.jpg\">CSDN</a>\n",
      "\t<a href=\"https://github.com/FontTian\">Name:Image 8<br/><img src=\"image8.jpg\">Github</a>\n",
      "\t<a href=\"http://www.cnblogs.com/fonttian/\">Name:Image 9<br/><img src=\"image9.jpg\">cnblogs</a>\n",
      "</div>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "\t<title>example4_2</title>\n",
      "\t<script type='text/javascript' src='/js/app1.js' />\n",
      "\t<script type='text/javascript' src='/js/app2.js' />\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "<div id=\"top\" class=\"div1\" style=\"width: 1230px\">\n",
      "\t<a href='/home.html'>主页</p>\n",
      "\t<a href=\"javascript:goToPage('/doc.html'); return false\">文档</a>\n",
      "\t<a href=\"javascript:goToPage('/example.html'); return false\">案例</a>\n",
      "\n",
      "</div>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scrapy.http import HtmlResponse\n",
    "html4_1 = open(htmlpath4_1).read()\n",
    "html4_2 = open(htmlpath4_2).read()\n",
    "\n",
    "print(html4_1)\n",
    "print(html4_2)\n",
    "\n",
    "response4_1 = HtmlResponse(url='http://example4_1.com',body=html4_1,encoding='utf8')\n",
    "response4_2 = HtmlResponse(url='http://example4_2.com',body=html4_2,encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先我们调用LinkExtractor来提取全部链接,之后我将介绍LinkExtractor中的各个参数.这些参数可以方便快捷的实现我们的很多想法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image1.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image2.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image3.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image4.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image5.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image6.html\n",
      "<class 'scrapy.link.Link'>  --->  http://blog.csdn.net/FontThrone\n",
      "<class 'scrapy.link.Link'>  --->  https://github.com/FontTian\n",
      "<class 'scrapy.link.Link'>  --->  http://www.cnblogs.com/fonttian/\n"
     ]
    }
   ],
   "source": [
    "from scrapy.linkextractors import LinkExtractor\n",
    "le = LinkExtractor()\n",
    "links = le.extract_links(response=response4_1)\n",
    "for link in links:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkExtractor中的参数\n",
    " - allow\n",
    "   接受一个正则表达式火一个正则表达式列表,提取绝对url与正则表达式匹配的链接,如果该参数位空(默认),则提取全部链接\n",
    " - **示例** 提取网页中所有以'/images'开头的链接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image1.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image2.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image3.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image4.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image5.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image6.html\n"
     ]
    }
   ],
   "source": [
    "pattern_allow = '/image[0-9]+\\.html$'\n",
    "le_allow = LinkExtractor(allow=pattern_allow)\n",
    "links_allow = le_allow.extract_links(response=response4_1)\n",
    "for link in links_allow:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - deny\n",
    "   接受一个正则表达式火一个正则表达式列表,与allow相反,排除绝对url与正则表达式匹配的链接.\n",
    " - **示例** 通过deny 来提取页面中所哟偶的站外链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^http://example4_1.com\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "pattern_deny = '^'+urlparse(response4_1.url).geturl()\n",
    "print(pattern_deny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://blog.csdn.net/FontThrone\n",
      "<class 'scrapy.link.Link'>  --->  https://github.com/FontTian\n",
      "<class 'scrapy.link.Link'>  --->  http://www.cnblogs.com/fonttian/\n"
     ]
    }
   ],
   "source": [
    "le_deny = LinkExtractor(deny=pattern_deny)\n",
    "links_deny = le_deny.extract_links(response=response4_1)\n",
    "for link in links_deny:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - allow_domains\n",
    "   接受一个域名或一个域名列表,提取到指定域的链接\n",
    " - **示例** 提取页面中所有到github以及csdn的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://blog.csdn.net/FontThrone\n",
      "<class 'scrapy.link.Link'>  --->  https://github.com/FontTian\n"
     ]
    }
   ],
   "source": [
    "domains = ['github.com','csdn.net']\n",
    "le_domains = LinkExtractor(allow_domains=domains)\n",
    "links_domains = le_domains.extract_links(response4_1)\n",
    "for link in links_domains:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - deny_domains\n",
    "   接受一个域名或一个域名列表,与allow_domains相反排除到指定域的链接\n",
    " - ** 示例 ** 提取页面中除了到github外所有的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image1.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image2.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image3.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image4.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image5.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image6.html\n",
      "<class 'scrapy.link.Link'>  --->  http://blog.csdn.net/FontThrone\n",
      "<class 'scrapy.link.Link'>  --->  http://www.cnblogs.com/fonttian/\n"
     ]
    }
   ],
   "source": [
    "le_deny_dommains = LinkExtractor(deny_domains='github.com')\n",
    "links_deny_dommains = le_deny_dommains.extract_links(response4_1)\n",
    "for link in links_deny_dommains:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - restrict_xpaths \n",
    "     接受一个XPath表达式或者一个XPatn表达式列表,提取Xpath表达式选中区域下的链接\n",
    " - **示例** 提取页面中```<div id = \"top\">```元素下的链接\n",
    "  - restrict_css\n",
    "     接受一个CSS选择器或者一个CSS选择器列表,提取CSS选择器选中区域下的链接\n",
    " - **示例** 提取页面中```<div id = \"bottom\">```元素下的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restrict_xpaths:\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image1.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image2.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image3.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image4.html\n",
      "\n",
      "restrict_css:\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image5.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_1.com/image6.html\n"
     ]
    }
   ],
   "source": [
    "print('restrict_xpaths:')\n",
    "\n",
    "le_xpath = LinkExtractor(restrict_xpaths='//div[@id=\"top\"]')\n",
    "links_xpath = le_xpath.extract_links(response4_1)\n",
    "for link in links_xpath:\n",
    "    print(type(link),\" ---> \",link.url)\n",
    "\n",
    "    \n",
    "print('\\nrestrict_css:')\n",
    "le_css = LinkExtractor(restrict_css='div#bottom')\n",
    "links_css = le_css.extract_links(response4_1)\n",
    "for link in links_css:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - tags\n",
    "   接受一个标签(字符串)或者一个标签列表,提取指定标签内的链,默认位['a'.'area']\n",
    " - attrs\n",
    "   接收一个属性(字符串)或一个属性列表,提取指定属性内的链接,默认为['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://example4_2.com/js/app1.js\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_2.com/js/app2.js\n"
     ]
    }
   ],
   "source": [
    "le_tags_attrs = LinkExtractor(tags = 'script',attrs = 'src')\n",
    "links_tags_attrs = le_tags_attrs.extract_links(response=response4_2)\n",
    "\n",
    "for link in links_tags_attrs:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - process_vlaue\n",
    "   接收一个形如func(vlaue)的回调函数,如果传递了该参数,LinkExtractor将调用该回调函数对提取每一个链接(如 a 的href)进行处理,回调函数正常情况下应该返回一个字符串的处理结果,想要抛弃所处理的连接时,则返回None.\n",
    " - **示例** 提取页面中的含义JavaScript代码的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapy.link.Link'>  --->  http://example4_2.com/home.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_2.com/doc.html\n",
      "<class 'scrapy.link.Link'>  --->  http://example4_2.com/example.html\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def process(value):\n",
    "    m  = re.search(\"javascript:goToPage\\('(.*?)'\",value)\n",
    "    # 如果匹配,就提取其中url并返回,如果不则返回原值\n",
    "    if m:\n",
    "        value = m.group(1)\n",
    "    return value\n",
    "\n",
    "le_process_value = LinkExtractor(process_value=process)\n",
    "links_process_value = le_process_value.extract_links(response4_2)\n",
    "for link in links_process_value:\n",
    "    print(type(link),\" ---> \",link.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
